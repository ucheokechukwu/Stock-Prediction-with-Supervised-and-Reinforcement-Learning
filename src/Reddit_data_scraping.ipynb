{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Lm-h6qUQGlZ_BPBjIrK-6TdPScoK8BuF",
      "authorship_tag": "ABX9TyMHFSRZlQINgkyZj6Zz0i2e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucheokechukwu/Stock-Prediction-with-Media-Sentiment-Analysis-/blob/main/src/Reddit_data_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "import praw\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "Jc84DYKdHsAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WTN6xArHxWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NlHT4GWESkLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'User-Agent': 'MyAPI/0.0.1'}"
      ],
      "metadata": {
        "id": "S2NAHWmDSn4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.post('https://www.reddit.com/api/v1/access_token',auth=auth,data=login_data, headers=headers)\n",
        "response.json()\n",
        "token = response.json()['access_token']\n",
        "headers = {**headers, **{'Authorization': f'bearer {token}'}}\n",
        "headers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3_Ul4-mSp7R",
        "outputId": "e08d5930-4273-49d3-e060-798bf888ef38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-Agent': 'MyAPI/0.0.1',\n",
              " 'Authorization': 'bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNjg2Nzg4OTI5LjUxODA5NiwiaWF0IjoxNjg2NzAyNTI5LjUxODA5NSwianRpIjoiTnptNFdnRG56RUY0amNfMHZzdTdTeTZWalg4RzB3IiwiY2lkIjoieGFTUVh0OVlLVDRSQ19IN3ViZVBHQSIsImxpZCI6InQyX2RjeDN4bmkyeSIsImFpZCI6InQyX2RjeDN4bmkyeSIsImxjYSI6MTY4NjcwMDE0NTQwMywic2NwIjoiZUp5S1Z0SlNpZ1VFQUFEX193TnpBU2MiLCJmbG8iOjl9.pn6smtnU_IfRmYQB_3_xmOk7dh4yiPOs5GF78zglEFXk7fLQH2DrjjXQEuq80NbLdV8VkU8MUNp9P1SfoAH6k_J89qlpTDlZb_D-SgHQho3YXmC7g91KNeIok7bv4Tg828BPCUUnLDlMpro68RiCkw9pUuCJhjiYlHTPrUJRVqAvGtEFUa6QWclP7T5Mz0n32_zL8JNoWRilvsFvDdpnvcGppT3rlm4bLBVSxhETxPebXPuEvTC_4-CETlQ_tTBOeYIDB-rdyfHBOA-lhYiJza-Ec3YNq_V6KBxESCDh5ZF6ALw1kjc2nb5RU4a3lSUdq0cMp2wnG4wyeuq_PxATpg'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import praw\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def collect_posts(subreddit_name, total_posts=10000, client_id=client_id, client_secret=secret_key, user_agent='MyAPI/0.0.1'):\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=client_id,\n",
        "        client_secret=client_secret,\n",
        "        user_agent=user_agent,\n",
        "    )\n",
        "\n",
        "    # List to store all the posts\n",
        "    all_posts = []\n",
        "\n",
        "    # Set to keep track of already collected post identifiers\n",
        "    collected_identifiers = set()\n",
        "\n",
        "    # List of different post sections to pull from\n",
        "    post_sections = ['hot', 'top', 'rising']\n",
        "\n",
        "    # Calculate the number of posts to pull from each section\n",
        "    total_posts_per_section = total_posts // len(post_sections)\n",
        "    remainder_posts = total_posts % len(post_sections)\n",
        "\n",
        "    # Progress bar for the overall progress\n",
        "    pbar = tqdm(total=total_posts)\n",
        "\n",
        "    for section in post_sections:\n",
        "        section_total_posts = total_posts_per_section\n",
        "        if remainder_posts > 0:\n",
        "            section_total_posts += 1\n",
        "            remainder_posts -= 1\n",
        "\n",
        "        try:\n",
        "            if section == 'random':\n",
        "                for _ in range(section_total_posts):\n",
        "                    submission = reddit.subreddit(subreddit_name).random()\n",
        "                    if submission.id not in collected_identifiers:\n",
        "                        all_posts.append(submission)\n",
        "                        collected_identifiers.add(submission.id)\n",
        "                        pbar.update(1)  # Update the progress bar for each collected post\n",
        "            else:\n",
        "                for submission in getattr(reddit.subreddit(subreddit_name), section)(limit=section_total_posts):\n",
        "                    if submission.id not in collected_identifiers:\n",
        "                        all_posts.append(submission)\n",
        "                        collected_identifiers.add(submission.id)\n",
        "                        pbar.update(1)  # Update the progress bar for each collected post\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting posts from {section}: {e}\")\n",
        "\n",
        "        if len(all_posts) >= total_posts:\n",
        "            break\n",
        "\n",
        "        time.sleep(0.5)  # Delay for 0.5 seconds between API requests\n",
        "\n",
        "        pbar.close()  # Close the progress bar\n",
        "\n",
        "    return all_posts"
      ],
      "metadata": {
        "id": "XW3vGxqHSrw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_posts_data(posts):\n",
        "    formatted_posts = []\n",
        "    for post in posts:\n",
        "        if isinstance(post, praw.models.Submission):\n",
        "            author_name = post.author.name if post.author else None\n",
        "            formatted_posts.append({\n",
        "                'post_date': pd.to_datetime(post.created_utc, unit='s'),\n",
        "                'kind': post.__class__.__name__,\n",
        "                # 'subreddit': post.subreddit.display_name,\n",
        "                'title': post.title,\n",
        "                'selftext': post.selftext,\n",
        "                # 'username': author_name,\n",
        "                # 'identifier': post.name,\n",
        "                'upvotes': post.ups,\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(formatted_posts)\n",
        "    return df"
      ],
      "metadata": {
        "id": "LCF6T2qjTqOi"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = collect_posts('netflix', total_posts=5)\n",
        "corpus_csv = extract_posts_data(tmp)\n",
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "\n",
        "filepath = '/content/drive/MyDrive/output/netflix_'+now.strftime('%d_%H_%M_%s')+'.csv'\n",
        "corpus_csv.to_csv(filepath, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78jtmSwMT9dc",
        "outputId": "4815ce47-cd25-42cc-c038-1d5f517f00e3"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting posts from hot: received 403 HTTP response\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting posts from top: received 403 HTTP response\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting posts from rising: received 403 HTTP response\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_csv"
      ],
      "metadata": {
        "id": "s5VOKINtT_Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_csv['post_date'].nunique()"
      ],
      "metadata": {
        "id": "z5uWChC2W3x_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a93b822-b777-4013-9039-e18439dbc889"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1414"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_csv.info()"
      ],
      "metadata": {
        "id": "NHJl2fIkX-rg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d0247e-c6b9-46c8-a58d-7b62b889bd89"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1414 entries, 0 to 1413\n",
            "Data columns (total 5 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   post_date  1414 non-null   datetime64[ns]\n",
            " 1   kind       1414 non-null   object        \n",
            " 2   title      1414 non-null   object        \n",
            " 3   selftext   1414 non-null   object        \n",
            " 4   upvotes    1414 non-null   int64         \n",
            "dtypes: datetime64[ns](1), int64(1), object(3)\n",
            "memory usage: 55.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RPLPtfZffpkd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}